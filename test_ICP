#繪製路徑
import numpy as np
import cv2 as cv
import open3d as o3d
from simpleicp import PointCloud, SimpleICP
from scipy.spatial import KDTree
import math
def save_xy_file(point_cloud, filename):
    # 提取点云数据的 x 和 y 坐标
    points_xy = np.asarray(point_cloud.points)[:, :2]

    # 将点云数据保存到 .xy 文件
    with open(filename, "w") as file:
        for point in points_xy:
            # 将每个点的 x、y 坐标写入文件，每行一个点
            file.write(f"{point[0]} {point[1]}\n")


MIN_MATCH_COUNT = 10

# 讀取img3
img3 = cv.imread("S__48734214.jpg" ,1) # trainImage
# 初始化 SIFT 檢測器
sift = cv.SIFT_create()

# 使用 SIFT 找到img3的關鍵點和描述子
kp3, des3 = sift.detectAndCompute(img3, None)


# 定義 FLANN 參數
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
search_params = dict(checks=50)

# 創建 FLANN 匹配器
flann = cv.FlannBasedMatcher(index_params, search_params)

# 儲存所有方框的座標
all_boxes = []

# 儲存所有中心點座標
center_points = []

#target特徵點坐標
target_points = []
#儲存特徵點坐標
matched_points = []
img = cv.imread("S__48734212.jpg", 1)  # queryImage

# 使用 SIFT 找到關鍵點和描述子
kp, des = sift.detectAndCompute(img, None)

# 計算每個關鍵點的旋轉量
rotations = [k.angle for k in kp]
rotationskp3 = [k.angle for k in kp3]
# 將旋轉量作為權重，計算加權平均
rotations = np.exp(rotations) / np.sum(np.exp(rotations))
rotations_des = np.sum(des * rotations[:, np.newaxis], axis=0)
rotationskp3 = np.exp(rotationskp3) / np.sum(np.exp(rotationskp3))
rotations_deskp3 = np.sum(des3 * rotationskp3[:, np.newaxis], axis=0)
# 將旋轉量轉換為弧度
rotations_rad = np.deg2rad(rotations_des)
rotations_radkp3 = np.deg2rad(rotations_deskp3)

# 將每個旋轉角度以單位向量表示並分成x與y分量
x = np.cos(rotations_rad)/np.linalg.norm(rotations_rad)
y = np.sin(rotations_rad)/np.linalg.norm(rotations_rad)
x3 = np.cos(rotations_radkp3)/np.linalg.norm(rotations_radkp3)
y3 = np.sin(rotations_radkp3)/np.linalg.norm(rotations_radkp3)
# 計算出最終角度
final_angle = np.arctan2(np.sum(y), np.sum(x))
final_angle_3 =np.arctan2(np.sum(y3), np.sum(x3))
# 轉換為角度
final_angle_deg = np.rad2deg(final_angle_3-final_angle)



print("sift rotational:")
print(rotations_des)
# 打印最終角度
print("The final angle is :")
print(final_angle_deg)
# # 進行特徵匹配
matches = flann.knnMatch(des, des3, k=2)

# 根據 Lowe's ratio 測試存儲所有良好的匹配
good = []
for m, n in matches:
    if m.distance < 0.7 * n.distance:
        good.append(m)
# 執行 Homography 變換
if len(good) > MIN_MATCH_COUNT:
    src_pts = np.float32([kp[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp3[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)
    matchesMask = mask.ravel().tolist()

    h, w, d = img.shape
    pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
    dst = cv.perspectiveTransform(pts, M)
    x1, y1 = np.int32(dst[0][0])
    x2, y2 = np.int32(dst[1][0])
    x3, y3 = np.int32(dst[2][0])
    x4, y4 = np.int32(dst[3][0])
    center_x = (x1 + x2 + x3 + x4) / 4
    center_y = (y1 + y2 + y3 + y4) / 4
    # print("中心坐標為:", center_x, center_y)
    center_points.append((center_x, center_y))
    matched_points.append(src_pts.squeeze())
    target_points.append(dst_pts.squeeze())
    all_boxes.append(np.int32(dst))

# 將所有方框繪製到img3上
color = (0, 255, 0)  # BGR
color2 = (0, 0, 255) # BGR

# 將特徵點的 x 和 y 坐標添加到點雲中，並將 z 坐標設置為 0
point_cloud = o3d.geometry.PointCloud()
target_point_cloud = o3d.geometry.PointCloud()
for points in matched_points:
    # 將每個特徵點轉換為 3D 點
    points_3d = np.hstack((points, np.zeros((points.shape[0], 1), dtype=np.float32)))
    # 將 3D 點添加到點雲中
    point_cloud.points.extend(o3d.utility.Vector3dVector(points_3d))
for points in target_points:
    # 將每個特徵點轉換為 3D 點
    target_points_3d = np.hstack((points, np.zeros((points.shape[0], 1), dtype=np.float32)))
    # 將 3D 點添加到點雲中
    target_point_cloud.points.extend(o3d.utility.Vector3dVector(target_points_3d))
save_xy_file(point_cloud,'point_cloud_test.xy')
save_xy_file(target_point_cloud,'target_point_cloud_test.xy')



def icp_2d(X, Y, max_iterations=100, tolerance=1e-5):
    # Initialize transformation
    R = np.eye(2)
    T = np.zeros(2)

    for _ in range(max_iterations):
        # Find nearest neighbors
        tree = KDTree(Y)
        distances, indices = tree.query(X)

        # Compute centroid
        X_centroid = np.mean(X, axis=0)
        Y_centroid = np.mean(Y[indices], axis=0)

        # Compute covariance matrix
        H = np.dot((X - X_centroid).T, (Y[indices] - Y_centroid))

        # Singular Value Decomposition
        U, _, Vt = np.linalg.svd(H)

        # Calculate rotation matrix
        R = np.dot(U, Vt)

        # Calculate translation vector
        T = Y_centroid - np.dot(R, X_centroid)

        # Apply transformation to source points
        X_transformed = np.dot(X, R.T) + T

        # Check convergence
        if np.all(np.abs(X_transformed - Y[indices]) < tolerance):
            break

        # Update source points
        X = X_transformed

    return R, T

# Example usage
# Define source and target point sets
X = np.loadtxt("point_cloud_test.xy")
Y = np.loadtxt("target_point_cloud_test.xy")

# Run ICP
R, T = icp_2d(X, Y)

def rotation_matrix_to_angle(R):
    angle = np.arctan2(R[1, 0], R[0, 0]) * 180 / np.pi
    return angle

# Convert rotation matrix to angle
rotation_angle = rotation_matrix_to_angle(R)

print("Rotation matrix:")
print(R)
print("Translation vector:")
print(T)
print("Rotation angle (degrees):")
print(rotation_angle)


# print(matched_points)
# print(target_points)



for box in all_boxes:
    img3 = cv.polylines(img3, [box], True, color, 3, cv.LINE_AA)
# 將所有中心點標示出來
for point in center_points:
    center_x, center_y = map(int, point)
    cv.circle(img3, (center_x, center_y), radius=5, color=(255, 255, 255), thickness=25)
# 將所有中心點連接成直線
for i in range(len(center_points) - 1):
    cv.line(img3, tuple(map(int, center_points[i])), tuple(map(int, center_points[i + 1])), color2, 10)
# 顯示結果
cv.namedWindow("All Boxes", cv.WINDOW_NORMAL)
cv.imshow("All Boxes", img3)
cv.waitKey(0)
cv.destroyAllWindows()
